{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatef-hosseinpour/dental-clf/blob/main/edl_3_clf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYTN_KJn3Q4s"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"dental_classification_edl.py\n",
        "\n",
        "Dental classification using Evidential Deep Learning for uncertainty quantification\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install edl_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from edl_pytorch import Dirichlet, evidential_classification\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set global configuration\n",
        "CONFIG = {\n",
        "    'IMAGE_SIZE': 224,\n",
        "    'BATCH_SIZE': 32,\n",
        "    'LEARNING_RATE': 0.001,\n",
        "    'EPOCHS': 30,\n",
        "    'SEED': 42,\n",
        "    'DEVICE': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    'NORMALIZATION_MEAN': [0.5, 0.5, 0.5],\n",
        "    'NORMALIZATION_STD': [0.5, 0.5, 0.5],\n",
        "    'SAVE_PATH': \"/content/drive/MyDrive/Dentisrty/models/\",\n",
        "    'METRICS_PATH': \"/content/drive/MyDrive/Dentisrty/metrics/\",\n",
        "    'LAMBDA_EDL': 1.0,  # EDL regularization parameter\n",
        "    'ANNEALING_COEFF': 0.01  # KL annealing coefficient\n",
        "}\n",
        "\n",
        "# Define the label mapping\n",
        "LABEL_MAPPING = {'amalgam_filling': 0, 'caries': 1, 'healthy': 2}\n",
        "NUM_CLASSES = len(LABEL_MAPPING)\n",
        "\n",
        "def seed_everything(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility across all libraries\"\"\"\n",
        "    import random\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed_everything(CONFIG['SEED'])\n",
        "\n",
        "# Define transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE'])),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CONFIG['NORMALIZATION_MEAN'], CONFIG['NORMALIZATION_STD'])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['IMAGE_SIZE'], CONFIG['IMAGE_SIZE'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CONFIG['NORMALIZATION_MEAN'], CONFIG['NORMALIZATION_STD'])\n",
        "])\n",
        "\n",
        "class DentalDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def load_data(data_dir):\n",
        "    \"\"\"Load images and labels from directories\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_counts = {label: 0 for label in LABEL_MAPPING.keys()}\n",
        "\n",
        "    for class_name, label_idx in LABEL_MAPPING.items():\n",
        "        folder_path = os.path.join(data_dir, class_name)\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            try:\n",
        "                image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "                if image is not None:\n",
        "                    # Convert BGR to RGB\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    image = Image.fromarray(image)\n",
        "                    images.append(image)\n",
        "                    labels.append(label_idx)\n",
        "                    label_counts[class_name] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    print(f\"Loaded class distribution: {label_counts}\")\n",
        "    return images, labels\n",
        "\n",
        "def plot_distribution(labels):\n",
        "    \"\"\"Plot the distribution of classes\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title('Distribution of Images by Class')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Images')\n",
        "\n",
        "    label_counts = {}\n",
        "    for label in labels:\n",
        "        if label in label_counts:\n",
        "            label_counts[label] += 1\n",
        "        else:\n",
        "            label_counts[label] = 1\n",
        "\n",
        "    x = list(LABEL_MAPPING.keys())\n",
        "    y = [label_counts.get(LABEL_MAPPING[key], 0) for key in x]\n",
        "\n",
        "    plt.bar(x, y)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "class EvidentialModel(nn.Module):\n",
        "    \"\"\"Evidential neural network model\"\"\"\n",
        "    def __init__(self, base_model, num_classes):\n",
        "        super(EvidentialModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Remove the original classifier\n",
        "        if hasattr(base_model, 'fc'):\n",
        "            # ResNet\n",
        "            feature_dim = base_model.fc.in_features\n",
        "            self.base_model.fc = nn.Identity()\n",
        "        elif hasattr(base_model, 'classifier'):\n",
        "            # DenseNet\n",
        "            feature_dim = base_model.classifier.in_features\n",
        "            self.base_model.classifier = nn.Identity()\n",
        "\n",
        "        # Evidential layer - outputs evidence (alpha - 1)\n",
        "        self.evidence_layer = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.base_model(x)\n",
        "        evidence = self.evidence_layer(features)\n",
        "        # Ensure evidence is positive using ReLU\n",
        "        evidence = torch.relu(evidence)\n",
        "        # Convert evidence to Dirichlet parameters (alphas)\n",
        "        alpha = evidence + 1\n",
        "        return alpha\n",
        "\n",
        "def build_evidential_models():\n",
        "    \"\"\"Build evidential ResNet18 and DenseNet121 models\"\"\"\n",
        "    # ResNet18\n",
        "    base_resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
        "    model_resnet18 = EvidentialModel(base_resnet18, NUM_CLASSES)\n",
        "\n",
        "    # DenseNet121\n",
        "    base_densenet121 = models.densenet121(weights='IMAGENET1K_V1')\n",
        "    model_densenet121 = EvidentialModel(base_densenet121, NUM_CLASSES)\n",
        "\n",
        "    return model_resnet18.to(CONFIG['DEVICE']), model_densenet121.to(CONFIG['DEVICE'])\n",
        "\n",
        "def evidential_loss(alpha, target, epoch, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Evidential loss function combining classification loss and KL divergence\n",
        "    \"\"\"\n",
        "    # Convert target to one-hot\n",
        "    target_one_hot = torch.zeros(alpha.size(0), alpha.size(1), device=device)\n",
        "    target_one_hot.scatter_(1, target.unsqueeze(1), 1)\n",
        "\n",
        "    # Sum of Dirichlet parameters\n",
        "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
        "\n",
        "    # Expected probability\n",
        "    p = alpha / S\n",
        "\n",
        "    # Accuracy term (negative log-likelihood)\n",
        "    A = torch.sum(target_one_hot * (torch.digamma(S) - torch.digamma(alpha)), dim=1)\n",
        "\n",
        "    # KL divergence term with annealing\n",
        "    annealing_coeff = min(1.0, epoch / (num_epochs * CONFIG['ANNEALING_COEFF']))\n",
        "\n",
        "    # KL divergence between Dirichlet and uniform distribution\n",
        "    beta = torch.ones((1, alpha.size(1)), device=device)\n",
        "    S_beta = torch.sum(beta, dim=1, keepdim=True)\n",
        "\n",
        "    lnB = torch.lgamma(S) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True)\n",
        "    lnB_beta = torch.sum(torch.lgamma(beta), dim=1, keepdim=True) - torch.lgamma(S_beta)\n",
        "\n",
        "    dg0 = torch.digamma(S)\n",
        "    dg1 = torch.digamma(alpha)\n",
        "\n",
        "    kl = lnB + lnB_beta + torch.sum((alpha - beta) * (dg1 - dg0), dim=1, keepdim=True)\n",
        "\n",
        "    # Total loss\n",
        "    loss = A + annealing_coeff * CONFIG['LAMBDA_EDL'] * kl.squeeze()\n",
        "\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def get_uncertainty_measures(alpha):\n",
        "    \"\"\"\n",
        "    Calculate various uncertainty measures from Dirichlet parameters\n",
        "    \"\"\"\n",
        "    # Total evidence\n",
        "    S = torch.sum(alpha, dim=1)\n",
        "\n",
        "    # Predicted probability (mean of Dirichlet)\n",
        "    prob = alpha / S.unsqueeze(1)\n",
        "\n",
        "    # Predictive uncertainty (total uncertainty)\n",
        "    predictive_uncertainty = NUM_CLASSES / S\n",
        "\n",
        "    # Aleatoric uncertainty (data uncertainty)\n",
        "    expected_prob = prob\n",
        "    aleatoric_uncertainty = torch.sum(expected_prob * (1 - expected_prob), dim=1)\n",
        "\n",
        "    # Epistemic uncertainty (model uncertainty)\n",
        "    epistemic_uncertainty = predictive_uncertainty - aleatoric_uncertainty\n",
        "\n",
        "    return {\n",
        "        'probability': prob,\n",
        "        'total_uncertainty': predictive_uncertainty,\n",
        "        'aleatoric_uncertainty': aleatoric_uncertainty,\n",
        "        'epistemic_uncertainty': epistemic_uncertainty,\n",
        "        'evidence': S - NUM_CLASSES\n",
        "    }\n",
        "\n",
        "def train_evidential_models(model_1, model_2, train_loader, model_1_optimizer, model_2_optimizer, epoch):\n",
        "    \"\"\"Train both evidential models for one epoch\"\"\"\n",
        "    model_1.train()\n",
        "    model_2.train()\n",
        "\n",
        "    total_model_1_loss = 0\n",
        "    total_model_2_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc='Training'):\n",
        "        inputs, labels = inputs.to(CONFIG['DEVICE']), labels.to(CONFIG['DEVICE'])\n",
        "\n",
        "        # Train model 1 (ResNet18)\n",
        "        model_1_optimizer.zero_grad()\n",
        "        alpha_1 = model_1(inputs)\n",
        "        model_1_loss = evidential_loss(alpha_1, labels, epoch, CONFIG['EPOCHS'], CONFIG['DEVICE'])\n",
        "        model_1_loss.backward()\n",
        "        model_1_optimizer.step()\n",
        "        total_model_1_loss += model_1_loss.item()\n",
        "\n",
        "        # Train model 2 (DenseNet121)\n",
        "        model_2_optimizer.zero_grad()\n",
        "        alpha_2 = model_2(inputs)\n",
        "        model_2_loss = evidential_loss(alpha_2, labels, epoch, CONFIG['EPOCHS'], CONFIG['DEVICE'])\n",
        "        model_2_loss.backward()\n",
        "        model_2_optimizer.step()\n",
        "        total_model_2_loss += model_2_loss.item()\n",
        "\n",
        "    return total_model_1_loss / num_batches, total_model_2_loss / num_batches\n",
        "\n",
        "def combine_evidential_predictions(alpha_1, alpha_2):\n",
        "    \"\"\"\n",
        "    Combine predictions from two evidential models using evidence combination\n",
        "    \"\"\"\n",
        "    # Convert alphas to evidence (subtract 1)\n",
        "    evidence_1 = alpha_1 - 1\n",
        "    evidence_2 = alpha_2 - 1\n",
        "\n",
        "    # Combine evidence by addition\n",
        "    combined_evidence = evidence_1 + evidence_2\n",
        "\n",
        "    # Convert back to alpha parameters\n",
        "    combined_alpha = combined_evidence + 1\n",
        "\n",
        "    return combined_alpha\n",
        "\n",
        "def evaluate_evidential_models(model_1, model_2, dataloader, phase=\"val\", visualize=False):\n",
        "    \"\"\"Evaluate using both evidential models\"\"\"\n",
        "    model_1.eval()\n",
        "    model_2.eval()\n",
        "\n",
        "    total_acc = 0\n",
        "    total_count = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_uncertainties = []\n",
        "    all_combined_alphas = []\n",
        "\n",
        "    label_idx_to_name = {v: k for k, v in LABEL_MAPPING.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=f'Evaluating {phase}')):\n",
        "            images, labels = images.to(CONFIG['DEVICE']), labels.to(CONFIG['DEVICE'])\n",
        "\n",
        "            # Get Dirichlet parameters from both models\n",
        "            alpha_1 = model_1(images)\n",
        "            alpha_2 = model_2(images)\n",
        "\n",
        "            # Combine predictions\n",
        "            combined_alpha = combine_evidential_predictions(alpha_1, alpha_2)\n",
        "\n",
        "            # Get uncertainty measures\n",
        "            uncertainty_measures = get_uncertainty_measures(combined_alpha)\n",
        "\n",
        "            # Make predictions\n",
        "            probs = uncertainty_measures['probability']\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            # Update metrics\n",
        "            correct = (preds == labels).sum().item()\n",
        "            total_acc += correct\n",
        "            total_count += len(labels)\n",
        "\n",
        "            # Store results\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_uncertainties.extend(uncertainty_measures['total_uncertainty'].cpu().numpy())\n",
        "            all_combined_alphas.extend(combined_alpha.cpu().numpy())\n",
        "\n",
        "            # Visualize results for first batch in test phase\n",
        "            if visualize and phase == \"test\" and batch_idx == 0:\n",
        "                visualize_evidential_results(images, uncertainty_measures, labels, batch_idx)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = total_acc / total_count\n",
        "    all_labels_np = np.array(all_labels)\n",
        "    all_preds_np = np.array(all_preds)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision_score(all_labels_np, all_preds_np, average='weighted'),\n",
        "        'recall': recall_score(all_labels_np, all_preds_np, average='weighted'),\n",
        "        'f1_score': f1_score(all_labels_np, all_preds_np, average='weighted'),\n",
        "        'confusion_matrix': confusion_matrix(all_labels_np, all_preds_np),\n",
        "        'mean_uncertainty': np.mean(all_uncertainties),\n",
        "        'std_uncertainty': np.std(all_uncertainties)\n",
        "    }\n",
        "\n",
        "    if phase == \"test\":\n",
        "        print_detailed_metrics(metrics)\n",
        "\n",
        "    return accuracy, metrics, all_combined_alphas\n",
        "\n",
        "def print_detailed_metrics(metrics):\n",
        "    \"\"\"Print detailed metrics for test evaluation\"\"\"\n",
        "    print(\"\\n----- Detailed Metrics -----\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "    print(f\"Mean Uncertainty: {metrics['mean_uncertainty']:.4f}\")\n",
        "    print(f\"Std Uncertainty: {metrics['std_uncertainty']:.4f}\")\n",
        "\n",
        "    # Display confusion matrix\n",
        "    cm = metrics['confusion_matrix']\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d',\n",
        "                xticklabels=list(LABEL_MAPPING.keys()),\n",
        "                yticklabels=list(LABEL_MAPPING.keys()))\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_evidential_results(images, uncertainty_measures, labels, batch_idx=0):\n",
        "    \"\"\"Visualize evidential prediction results with uncertainty\"\"\"\n",
        "    num_images = min(16, len(images))\n",
        "    grid_cols = 4\n",
        "    grid_rows = math.ceil(num_images / grid_cols)\n",
        "\n",
        "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, 4 * grid_rows))\n",
        "    if grid_rows == 1 and grid_cols == 1:\n",
        "        axes = np.array([axes])\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Get mapping from index to label name\n",
        "    label_idx_to_name = {v: k for k, v in LABEL_MAPPING.items()}\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i < num_images:\n",
        "            # Get image\n",
        "            img = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "            img = img * np.array(CONFIG['NORMALIZATION_STD']) + np.array(CONFIG['NORMALIZATION_MEAN'])\n",
        "            img = np.clip(img, 0, 1)\n",
        "\n",
        "            # Get predictions and uncertainties\n",
        "            probs = uncertainty_measures['probability'][i].cpu().numpy()\n",
        "            pred_idx = np.argmax(probs)\n",
        "            pred_class = label_idx_to_name[pred_idx]\n",
        "            true_class = label_idx_to_name[labels[i].item()]\n",
        "\n",
        "            total_unc = uncertainty_measures['total_uncertainty'][i].cpu().item()\n",
        "            aleatoric_unc = uncertainty_measures['aleatoric_uncertainty'][i].cpu().item()\n",
        "            epistemic_unc = uncertainty_measures['epistemic_uncertainty'][i].cpu().item()\n",
        "\n",
        "            # Format text\n",
        "            prob_text = \"\\n\".join([f\"{label_idx_to_name[j]}: {probs[j]:.3f}\"\n",
        "                                  for j in range(len(probs))])\n",
        "            unc_text = f\"Total: {total_unc:.3f}\\nAleatoric: {aleatoric_unc:.3f}\\nEpistemic: {epistemic_unc:.3f}\"\n",
        "\n",
        "            # Show image with predictions\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"Pred: {pred_class}\\nTrue: {true_class}\\n\\nProbs:\\n{prob_text}\\n\\nUncertainty:\\n{unc_text}\",\n",
        "                        fontsize=8)\n",
        "\n",
        "            # Highlight incorrect predictions or high uncertainty\n",
        "            if pred_class != true_class or total_unc > 0.5:\n",
        "                color = 'red' if pred_class != true_class else 'orange'\n",
        "                for spine in ax.spines.values():\n",
        "                    spine.set_edgecolor(color)\n",
        "                    spine.set_linewidth(3)\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'edl_prediction_batch_{batch_idx}.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_uncertainty_distribution(uncertainties, labels, save_path):\n",
        "    \"\"\"Plot uncertainty distribution by class\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    label_idx_to_name = {v: k for k, v in LABEL_MAPPING.items()}\n",
        "\n",
        "    # Plot uncertainty by class\n",
        "    plt.subplot(2, 2, 1)\n",
        "    for class_idx in range(NUM_CLASSES):\n",
        "        class_mask = np.array(labels) == class_idx\n",
        "        class_uncertainties = np.array(uncertainties)[class_mask]\n",
        "        plt.hist(class_uncertainties, alpha=0.7, label=label_idx_to_name[class_idx], bins=20)\n",
        "    plt.xlabel('Total Uncertainty')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Uncertainty Distribution by Class')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot uncertainty vs accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    correct_predictions = np.array(labels) == np.argmax(uncertainties, axis=1) if len(np.array(uncertainties).shape) > 1 else np.ones_like(labels)\n",
        "    plt.scatter(uncertainties, correct_predictions, alpha=0.6)\n",
        "    plt.xlabel('Total Uncertainty')\n",
        "    plt.ylabel('Correct Prediction')\n",
        "    plt.title('Uncertainty vs Prediction Correctness')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, 'uncertainty_analysis.png'))\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Set the data directory\n",
        "    data_dir = '/content/drive/MyDrive/Dentisrty/panoramic_data'\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    images, labels = load_data(data_dir)\n",
        "    print(f\"Loaded {len(images)} images\")\n",
        "\n",
        "    # Plot class distribution\n",
        "    plot_distribution(labels)\n",
        "\n",
        "    # Split data\n",
        "    print(\"Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        images, labels, test_size=0.2, random_state=CONFIG['SEED']\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=CONFIG['SEED']\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    print(\"Creating datasets and dataloaders...\")\n",
        "    train_dataset = DentalDataset(X_train, y_train, transform=train_transforms)\n",
        "    val_dataset = DentalDataset(X_val, y_val, transform=val_transforms)\n",
        "    test_dataset = DentalDataset(X_test, y_test, transform=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Initialize evidential models\n",
        "    print(\"Initializing evidential models...\")\n",
        "    model_resnet18, model_densenet121 = build_evidential_models()\n",
        "\n",
        "    # Optimizers\n",
        "    model_1_optimizer = optim.AdamW(model_resnet18.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=1e-4)\n",
        "    model_2_optimizer = optim.AdamW(model_densenet121.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=1e-4)\n",
        "\n",
        "    # Create schedulers\n",
        "    model_1_scheduler = optim.lr_scheduler.CosineAnnealingLR(model_1_optimizer, T_max=CONFIG['EPOCHS'])\n",
        "    model_2_scheduler = optim.lr_scheduler.CosineAnnealingLR(model_2_optimizer, T_max=CONFIG['EPOCHS'])\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Starting evidential training for {CONFIG['EPOCHS']} epochs\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    best_val_accuracy = 0.0\n",
        "    metrics_history = []\n",
        "\n",
        "    for epoch in range(CONFIG['EPOCHS']):\n",
        "        print(f'\\nEpoch {epoch+1}/{CONFIG[\"EPOCHS\"]}')\n",
        "\n",
        "        # Train\n",
        "        model_1_loss, model_2_loss = train_evidential_models(\n",
        "            model_resnet18, model_densenet121, train_loader,\n",
        "            model_1_optimizer, model_2_optimizer, epoch\n",
        "        )\n",
        "\n",
        "        # Step schedulers\n",
        "        model_1_scheduler.step()\n",
        "        model_2_scheduler.step()\n",
        "\n",
        "        print(f'ResNet18 EDL Loss: {model_1_loss:.4f}, DenseNet121 EDL Loss: {model_2_loss:.4f}')\n",
        "\n",
        "        # Validate\n",
        "        val_accuracy, val_metrics, _ = evaluate_evidential_models(\n",
        "            model_resnet18, model_densenet121, val_loader, \"val\"\n",
        "        )\n",
        "        print(f'Validation Accuracy: {val_accuracy:.4f}, Mean Uncertainty: {val_metrics[\"mean_uncertainty\"]:.4f}')\n",
        "\n",
        "        # Save best models\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model_resnet18.state_dict(), os.path.join(CONFIG['SAVE_PATH'], 'best_resnet18_edl.pth'))\n",
        "            torch.save(model_densenet121.state_dict(), os.path.join(CONFIG['SAVE_PATH'], 'best_densenet121_edl.pth'))\n",
        "            print(f'Saved new best models with validation accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        # Track metrics\n",
        "        metrics_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'resnet18_loss': model_1_loss,\n",
        "            'densenet121_loss': model_2_loss,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'val_precision': val_metrics['precision'],\n",
        "            'val_recall': val_metrics['recall'],\n",
        "            'val_f1': val_metrics['f1_score'],\n",
        "            'val_mean_uncertainty': val_metrics['mean_uncertainty']\n",
        "        })\n",
        "\n",
        "    # Save training history\n",
        "    pd.DataFrame(metrics_history).to_csv(os.path.join(CONFIG['METRICS_PATH'], 'edl_training_history.csv'), index=False)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot([m['epoch'] for m in metrics_history], [m['resnet18_loss'] for m in metrics_history], label='ResNet18 Loss')\n",
        "    plt.plot([m['epoch'] for m in metrics_history], [m['densenet121_loss'] for m in metrics_history], label='DenseNet121 Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot([m['epoch'] for m in metrics_history], [m['val_accuracy'] for m in metrics_history], label='Validation Accuracy')\n",
        "    plt.plot([m['epoch'] for m in metrics_history], [m['val_precision'] for m in metrics_history], label='Validation Precision')\n",
        "    plt.plot([m['epoch'] for m in metrics_history], [m['val_f1'] for m in metrics_history], label='Validation F1')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot([m['epoch'] for m in metrics_history], [m['val_mean_uncertainty'] for m in metrics_history], label='Mean Uncertainty')\n",
        "    plt.title('Validation Mean Uncertainty')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Uncertainty')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CONFIG['METRICS_PATH'], 'edl_training_history.png'))\n",
        "    plt.show()\n",
        "\n",
        "    # Final evaluation\n",
        "    print('\\nPerforming final evidential evaluation...')\n",
        "    test_accuracy, test_metrics, final_alphas = evaluate_evidential_models(\n",
        "        model_resnet18, model_densenet121, test_loader, \"test\", visualize=True\n",
        "    )\n",
        "\n",
        "    # Print final results\n",
        "    print(50 * '=')\n",
        "    print('FINAL EVIDENTIAL RESULTS:')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "    print(f'Test Precision: {test_metrics[\"precision\"]:.4f}')\n",
        "    print(f'Test Recall: {test_metrics[\"recall\"]:.4f}')\n",
        "    print(f'Test F1 Score: {test_metrics[\"f1_score\"]:.4f}')\n",
        "    print(f'Mean Test Uncertainty: {test_metrics[\"mean_uncertainty\"]:.4f}')\n",
        "    print(f'Std Test Uncertainty: {test_metrics[\"std_uncertainty\"]:.4f}')\n",
        "    print(50 * '=')\n",
        "\n",
        "    # Save final results\n",
        "    results_df = pd.DataFrame({\n",
        "        'Model': ['EDL ResNet18+DenseNet121'],\n",
        "        'Accuracy': [test_accuracy],\n",
        "        'Precision': [test_metrics['precision']],\n",
        "        'Recall': [test_metrics['recall']],\n",
        "        'F1_Score': [test_metrics['f1_score']],\n",
        "        'Mean_Uncertainty': [test_metrics['mean_uncertainty']],\n",
        "        'Std_Uncertainty': [test_metrics['std_uncertainty']]\n",
        "    })\n",
        "    results_df.to_csv(os.path.join(CONFIG['METRICS_PATH'], 'edl_final_results.csv'), index=False)\n",
        "\n",
        "    print(f\"Results saved to {os.path.join(CONFIG['METRICS_PATH'], 'edl_final_results.csv')}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}